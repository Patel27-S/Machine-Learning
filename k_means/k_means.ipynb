{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6930d695-757a-4f5b-afeb-f628a5029426",
   "metadata": {},
   "source": [
    "### Imports neccesary for performing k-means :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "92c770ce-6eeb-4705-a393-f4d35a6635d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.metrics.cluster import adjusted_rand_score, rand_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d93033-e803-4c6e-80da-d4bbe0f733c2",
   "metadata": {},
   "source": [
    "### Loading the fashion_mnist_dataset and also one more, for performing k-means clustering as mentioned in the questions of the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "8b4525ff-7562-4ab7-b0db-b60a5d9654ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the .csv file into a Pandas' dataframe object :\n",
    "fashion_mnist_dataset = pd.read_csv('fashion-mnist_train.csv')\n",
    "\n",
    "# Fetching the class label column out of the 'fashion_mnist_dataset':\n",
    "fashion_mnist_true_labels = fashion_mnist_dataset['label']\n",
    "\n",
    "# Now, removing the first column/attribute which is a 'label':\n",
    "fashion_mnist_dataset = fashion_mnist_dataset.drop(['label'], axis=1)\n",
    "\n",
    "# Converting the pd.DataFrame object into a numpy array object :\n",
    "fashion_mnist_dataset = np.array(fashion_mnist_dataset)\n",
    "\n",
    "# We also need a separate dataset for the Question.1 of the assignment in order to implement the \n",
    "# algorithm first with it, hence :\n",
    "\n",
    "X,y = make_blobs(n_samples=200,n_features=2,random_state=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb63130-8e57-4afe-aa96-a2620a72c67b",
   "metadata": {},
   "source": [
    "### K-Means :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a28a38a2-1510-4c55-9f38-0b05e6607853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm for easily having a track for the work below :\n",
    "\n",
    "# Step 1 : We want to know how many clusters do we want to form for the given dataset's clustering :\n",
    "\n",
    "# Step 2 : We want to select the random 'k' datapoints from the dataset or the high dimensional space \n",
    "#          if we choose to have 'k' clusters getting formed for the dataset that we have.\n",
    "\n",
    "# Step 3 : Assigning the datapoints of the dataset to the centroids(or randomly selected points in Step 2(i.e. also known as \n",
    "#          clusters)). This is done based upon the centroid which is nearest among all the centroids \n",
    "#          for a particular datapoint in the dataset.\n",
    "\n",
    "# Step 4 : Determining new centroids from the clusters formed. (Each New centroids are nothing but the mean of the clusters)\n",
    "\n",
    "# Step 5 : Checking if the new centroids are same as the previous centroids. And, if found so then it marks the completion. \n",
    "#          And, if not so, then we re-assign the clusters to the datapoints of the dataset. And, re-calculate the new centroids\n",
    "#          after which also check if they are the same or almost same as the previous centroids. (i.e. In short, repeat Step.3, \n",
    "#          Step.4 and Step.5).\n",
    "\n",
    "\n",
    "class KMeans:\n",
    "    \n",
    "    # While instantiating the KMeans Object, the constructor would be \n",
    "    # passed with the parameters of n_clusters and max_iterations to be \n",
    "    # performed. By default values of n_clusters and max_iter is as mentioned :-\n",
    "    def __init__(self,number_clusters=3,max_iter=500):\n",
    "        self.number_clusters = number_clusters\n",
    "        self.max_iter = max_iter\n",
    "        self.centroids = None\n",
    "\n",
    "        \n",
    "    def performer_function(self,dataset):\n",
    "        '''\n",
    "        This function will take the input parameter as a 'dataset' which would \n",
    "        be a numpy array object.\n",
    "        \n",
    "        It will perform the task of Randomly assigning centroids \n",
    "        to be used for cluster formation, then assigning the datapoints of the dataset to the \n",
    "        clusters(or centroids) that they are closest to, Determining the new centroids, and \n",
    "        checking if the algorithm has converged(i.e. if the new centroids\n",
    "        are the same as the old ones or almost like them).\n",
    "        \n",
    "        The return value would be the result :- numpy array object of the number \n",
    "        of rows as same as 'dataset', with each particular element in it determining the cluster \n",
    "        assigned to the corresponding datapoint in the 'dataset'.\n",
    "        \n",
    "        '''\n",
    "        # Selecting the 'n_clusters' numbered random datapoints to start the clustering the datapoints of \n",
    "        # 'dataset'.\n",
    "        random_indexes = random.sample(range(0,dataset.shape[0]),self.number_clusters)\n",
    "        self.centroids = dataset[random_indexes]\n",
    "        \n",
    "        for i in range(self.max_iter):\n",
    "            \n",
    "            # Assigning Clusters to the datapoints of the 'dataset'.\n",
    "            # Helper function \"assign_clusters()\" is used which is defined below.\n",
    "            cluster_group = self.assign_clusters(dataset)\n",
    "            \n",
    "             \n",
    "            previous_centroids = self.centroids\n",
    "            \n",
    "            # Determining the new centroids using the Helper function 'move_centroids()' \n",
    "            # function which is defined below :\n",
    "            self.centroids = self.move_centroids(dataset,cluster_group)\n",
    "            \n",
    "            # Checking if the algorithm is converged :(i.e. If the Old centroids and \n",
    "            # New centroids are same)\n",
    "            if (previous_centroids == self.centroids).all():\n",
    "                # If the condition is true then it's a convergence.\n",
    "                break\n",
    "\n",
    "        return cluster_group\n",
    "\n",
    "    \n",
    "    \n",
    "    def assign_clusters(self,dataset):\n",
    "        '''\n",
    "        Returns a numpy array object of the same number of rows as the \n",
    "        input 'dataset'.\n",
    "        \n",
    "        In the returned array, each element indicates the group assigned to the \n",
    "        corresponding element/datapoint in the 'dataset'.\n",
    "        '''\n",
    "        cluster_group = []\n",
    "        distances = []\n",
    "        \n",
    "        # For each row in the dataset(which is a numpy array object), we calculate its \n",
    "        # euclidean distance with each of the centroids and assign it to the centroid\n",
    "        # (or the cluster group) to which it is the nearest.\n",
    "        for row in dataset:\n",
    "            \n",
    "            for centroid in self.centroids:\n",
    "                distances.append(np.sqrt(np.dot(row-centroid,row-centroid)))\n",
    "            \n",
    "            # Finding the minimum of distances :\n",
    "            min_distance = min(distances)\n",
    "            # Finding the index position corresponding to the minimum distance value :\n",
    "            index_position = distances.index(min_distance)\n",
    "            # Appending the assigned group to the 'cluster_group' list :\n",
    "            cluster_group.append(index_position)\n",
    "            # Clearing the distance for the next datapoint's similar process :\n",
    "            distances.clear()\n",
    "\n",
    "        return np.array(cluster_group)\n",
    "\n",
    "    \n",
    "    \n",
    "    def move_centroids(self,dataset,cluster_group):\n",
    "        '''\n",
    "        This function takes in the input parameters as the dataset(numpy array object) \n",
    "        and a cluster_group which is of the same size in terms of rows as the dataset.\n",
    "        \n",
    "        And, also is determining cluster group assigned to the datapoints in the 'dataset'.\n",
    "        '''\n",
    "        new_centroids = []\n",
    "\n",
    "        cluster_type = np.unique(cluster_group)\n",
    "\n",
    "        for type in cluster_type:\n",
    "            new_centroids.append(dataset[cluster_group == type].mean(axis=0))\n",
    "\n",
    "        return np.array(new_centroids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8afbc05-5a82-4129-9d91-4a60bcd2254e",
   "metadata": {},
   "source": [
    "### Using the above defined class for performing clustering on the dataset prepared using the make_blobs() function, until the convergence happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b8d0ec6b-0533-4fc6-9ef1-371adb0d5331",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,\n",
       "       0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,\n",
       "       1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,\n",
       "       0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1,\n",
       "       1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,\n",
       "       0, 0])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiating the KMeans class:\n",
    "k = KMeans(number_clusters = 2, max_iter = 500)\n",
    "\n",
    "# Using the performer_function():\n",
    "# Passing the 'X' dataset, which is already formed using \n",
    "# make_blobs() function :\n",
    "result_for_X = k.performer_function(X)\n",
    "\n",
    "result_for_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d13da775-7749-49e8-abbb-e36478980f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, to measure the performance of clustering of the dataset, we can either plot \n",
    "# the dataset( if with fewer dimensions such as 2 or 3)\n",
    "# and have a look OR use RAND index, adjusted RAND index to determine the performance \n",
    "# of clustering for different values of 'k' (i.e. the number of clusters), given the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92410eb-a5b9-40d8-b01a-fe7f0719e29d",
   "metadata": {},
   "source": [
    "### Trying the 'fashion_mnist_dataset'. For, different values of k ranging from 5 to 15 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c55f76aa-bb07-4680-b343-3b56b8604aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "performance_adj_rand_ 5 : 0.2857359159682132\n",
      "performance_rand_ 5 : 0.81182257926521\n",
      "\n",
      "performance_adj_rand_ 6 : 0.32548930863700093\n",
      "performance_rand_ 6 : 0.8409968638366195\n",
      "\n",
      "performance_adj_rand_ 7 : 0.33366222987843075\n",
      "performance_rand_ 7 : 0.849844765190531\n",
      "\n",
      "performance_adj_rand_ 8 : 0.3383150506412536\n",
      "performance_rand_ 8 : 0.8585529281043573\n",
      "\n",
      "performance_adj_rand_ 9 : 0.35992692982489277\n",
      "performance_rand_ 9 : 0.8691129768829481\n",
      "\n",
      "performance_adj_rand_ 10 : 0.35260966769213675\n",
      "performance_rand_ 10 : 0.8765005533425557\n",
      "\n",
      "performance_adj_rand_ 11 : 0.35697086165973047\n",
      "performance_rand_ 11 : 0.8819256259826552\n",
      "\n",
      "performance_adj_rand_ 12 : 0.32864881783747973\n",
      "performance_rand_ 12 : 0.8793135018916982\n",
      "\n",
      "performance_adj_rand_ 13 : 0.35169788888738124\n",
      "performance_rand_ 13 : 0.8894960510452952\n",
      "\n",
      "performance_adj_rand_ 14 : 0.3304629385562239\n",
      "performance_rand_ 14 : 0.8866655733151108\n",
      "\n",
      "performance_adj_rand_ 15 : 0.3539219701378049\n",
      "performance_rand_ 15 : 0.8959657916520831\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5, 16):\n",
    "   \n",
    "    kmeans = KMeans(number_clusters= i, max_iter = 20000)\n",
    "    prediction_clustering = kmeans.performer_function(fashion_mnist_dataset)\n",
    "\n",
    "    performance_adj_rand = adjusted_rand_score(fashion_mnist_true_labels, prediction_clustering)\n",
    "    performance_rand = rand_score(fashion_mnist_true_labels, prediction_clustering)\n",
    "\n",
    "    print('performance_adj_rand_',i,':', performance_adj_rand)\n",
    "    print('performance_rand_',i,':', performance_rand)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd37750-5307-4f12-9eb5-098698e6ee6e",
   "metadata": {},
   "source": [
    "### Getting the best value of rand_index for k=15. Hence, implementing the KMeans on the dataset for k=15, 5 times for random initializations each time :- (Implementing in a normal manner as the initializations will with a high probability be changing almost each time, due to use of 'random()' function above) :-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "ef38ea44-7abf-4078-9f2f-4f6e28874654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 th time\n",
      "performance_adj_rand_ 15 : 0.37369075558706233\n",
      "performance_rand_ 15 : 0.9001197331066629\n",
      "\n",
      "1 th time\n",
      "performance_adj_rand_ 15 : 0.389030289923332\n",
      "performance_rand_ 15 : 0.9036660455452036\n",
      "\n",
      "2 th time\n",
      "performance_adj_rand_ 15 : 0.3731838285208089\n",
      "performance_rand_ 15 : 0.9011280149113596\n",
      "\n",
      "3 th time\n",
      "performance_adj_rand_ 15 : 0.374692593757079\n",
      "performance_rand_ 15 : 0.9014028217136952\n",
      "\n",
      "4 th time\n",
      "performance_adj_rand_ 15 : 0.3752691572696169\n",
      "performance_rand_ 15 : 0.9014777551848087\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for j in range(0,5):\n",
    "    \n",
    "    for i in range(15,16):\n",
    "\n",
    "        kmeans = KMeans(number_clusters= i, max_iter = 20000)\n",
    "        prediction_clustering = kmeans.performer_function(fashion_mnist_dataset)\n",
    "\n",
    "        performance_adj_rand = adjusted_rand_score(fashion_mnist_true_labels, prediction_clustering)\n",
    "        performance_rand = rand_score(fashion_mnist_true_labels, prediction_clustering)\n",
    "        \n",
    "        print(j, 'th time')\n",
    "        print('performance_adj_rand_',i,':', performance_adj_rand)\n",
    "        print('performance_rand_',i,':', performance_rand)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99c7746-1e75-47ce-8df6-c5044a200f32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
